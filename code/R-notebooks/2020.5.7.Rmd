---
title: Normalizing 16S data using DESeq
author: Sumeed Manzoor<br>[Chang Lab](https://changlab.uchicago.edu/) at the University of Chicago
date: May 6, 2020, *updated `r format(Sys.Date(), '%b %d, %Y')`*
output:
  html_document:
    theme: sandstone
    highlight: textmate
    code_folding: "show"
    toc: true
    toc_float:
        collapsed: true
    toc_depth: 3
    includes:
        after_body: footer.html
---

<a href="http://yoyomanzoor.github.io/Chang-Lab-Notebook/" class="fa fa-home"></a>

<a href="https://changlab.uchicago.edu/" alt="The Chang Lab at the University of Chicago" target="_blank"><img src="https://img.shields.io/badge/%E2%80%8F%E2%80%8F%E2%80%8E-Chang%20Lab-blue?style=flat-square&logo=jekyll" /></a>
<a href="https://github.com/Yoyomanzoor/Katya-Microbiome" alt="Github Repo" target="_blank"><img src="https://img.shields.io/badge/%E2%80%8F%E2%80%8F%E2%80%8E-github%20repo-blueviolet?style=flat-square&logo=github" /></a>
<a href="" alt="tag" target="_blank"><img src="https://img.shields.io/badge/tags-programming%20|%20data_exploration%20|%20rmarkdown%20|%20microbiome-lightgrey?style=flat-square" /></a>

---

*This page contains a lot of datatables and may take a minute to load (15.4 MB)*

Please see the paper<a href="https://www.ncbi.nlm.nih.gov/pubmed/25516281" class="ai ai-pubmed"></a>, [vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html), [manual](https://bioconductor.org/packages/release/bioc/manuals/DESeq2/man/DESeq2.pdf), and [vst notebook](http://www.bioconductor.org/packages//2.13/bioc/vignettes/DESeq/inst/doc/vst.pdf) for much greater detail on specific aspects of DESeq2.

# libraries

```{r setup}
reqpkg <- c("DESeq2","dplyr","magrittr","DT","vsn","pheatmap","RColorBrewer","ggplot2","ggpubr")
for (i in reqpkg) {
	print(i)
	print(packageVersion(i))
	suppressWarnings(suppressMessages(library(i, quietly=T, verbose=T, warn.conflicts=F,character.only=T)))
}

theme_set(theme_pubr())
```

# import data {.tabset .tabset-pills}

```{r}
df <- read.csv2("table_dada2_mfilt_p6364_feature10_count.tsv", sep="\t")

metadata <- read.csv2("48hr_metadata.csv", sep=",")

# pulling sample numbers from column names of df. Accomplished by repeated string splits.
metadata$SampleNo <- strsplit(colnames(df[,-1]), "no") %>% sapply("[", 2) %>% strsplit("zt\\d") %>% sapply("[[", 1) %>% gsub(".{1}$", "", .) %>% as.numeric()

metadata %>% datatable()
```

Some functions to extract and display a sample from assay data.

```{r}
dtable <- function(d, n) {
	d[, c(1, which(metadata$SampleNo == n)+1)] %>%
		datatable(filter="bottom")
}
dtable2 <- function(d, n) {
	d[, which(metadata$SampleNo == n)] %>%
		datatable(filter="bottom")
}
```

Taking a look at the data via datatables.

## no1

```{r}
dtable(df,1)
```

## no3

```{r}
dtable(df,3)
```
## no9

```{r}
dtable(df,9)
```

## no10

```{r}
dtable(df,10)
```

## no15

```{r}
dtable(df,15)
```

## no16

```{r}
dtable(df,16)
```

## no17

```{r}
dtable(df,17)
```

## no18

```{r}
dtable(df,18)
```

## no19

```{r}
dtable(df,19)
```

## no58

```{r}
dtable(df,58)
```

## no60

```{r}
dtable(df,60)
```

## no61

```{r}
dtable(df,61)
```

## no63

```{r}
dtable(df,63)
```

## no65

```{r}
dtable(df,65)
```

## no66

```{r}
dtable(df,66)
```

## no67

```{r}
dtable(df,67)
```

## no69

```{r}
dtable(df,69)
```

## no70

```{r}
dtable(df,70)
```

## no71

```{r}
dtable(df,71)
```

## no72

```{r}
dtable(df,72)
```

## no75

```{r}
dtable(df,75)
```

## no120

```{r}
dtable(df,120)
```

## no121

```{r}
dtable(df,121)
```

## no122

```{r}
dtable(df,122)
```

## no125

```{r}
dtable(df,125)
```

## no126

```{r}
dtable(df,126)
```

## no127

```{r}
dtable(df,127)
```

## no131

```{r}
dtable(df,131)
```

## no136

```{r}
dtable(df,136)
```

## no151

```{r}
dtable(df,151)
```

## no153

```{r}
dtable(df,153)
```

# generate DESeq object

```{r}
df %<>%
	set_rownames(.$X.OTU.ID) %>%
	select(-X.OTU.ID)

condition_list <- data.frame(row.names = colnames(df),
			     Gender = relevel(metadata$Gender, "M"),
			     Genotype = relevel(metadata$Genotype, "WT"),
			     Time = factor(metadata$Time)
)

dds <- DESeq2::DESeqDataSetFromMatrix(countData = df,
				      colData = condition_list,
				      design = ~ Gender + Genotype + Gender:Genotype
)

dds <- estimateSizeFactors(dds)

# Creating DESeqTransform object from raw data to compare with transformations downstream
dds_dud <- SummarizedExperiment(counts(dds), colData = colData(dds)) %>%
  DESeqTransform()
```

# normalization {.tabset .tabset-pills}

I'll attempt 4 types of normalization by DESeq2

- log2 normalized to estimated size factors (median of the ratios)
- variance stabilization
- regularized log
- log10 scaling only

Of these, all besides log10 scaling are normalizations that consider library size.

## log2 normalized to estimated size factors (ntd) {.tabset .tabset-pills}

This is $\log_2 (n + 1)$, where $n$ is normalized to estimated size factors. However, since counts data tends to prioritize available normalization factors before defaulting to estimated size factors, I'll first check the available normalization factors and estimated size factors in the DESeqData object.  
The result of this function are _psuedocounts_, basically just scaled counts.

```{r}
normalizationFactors(dds)
sizeFactors(dds) %>% head()
ntd <- normTransform(dds)
ntd_assay <- assay(ntd)
colData(ntd)
```

Below are tables for every sample's ntd output.

### no1

```{r}
dtable2(ntd_assay, 1)
```

### no3

```{r}
dtable2(ntd_assay,3)
```
### no9

```{r}
dtable2(ntd_assay,9)
```

### no10

```{r}
dtable2(ntd_assay,10)
```

### no15

```{r}
dtable2(ntd_assay,15)
```

### no16

```{r}
dtable2(ntd_assay,16)
```

### no17

```{r}
dtable2(ntd_assay,17)
```

### no18

```{r}
dtable2(ntd_assay,18)
```

### no19

```{r}
dtable2(ntd_assay,19)
```

### no58

```{r}
dtable2(ntd_assay,58)
```

### no60

```{r}
dtable2(ntd_assay,60)
```

### no61

```{r}
dtable2(ntd_assay,61)
```

### no63

```{r}
dtable2(ntd_assay,63)
```

### no65

```{r}
dtable2(ntd_assay,65)
```

### no66

```{r}
dtable2(ntd_assay,66)
```

### no67

```{r}
dtable2(ntd_assay,67)
```

### no69

```{r}
dtable2(ntd_assay,69)
```

### no70

```{r}
dtable2(ntd_assay,70)
```

### no71

```{r}
dtable2(ntd_assay,71)
```

### no72

```{r}
dtable2(ntd_assay,72)
```

### no75

```{r}
dtable2(ntd_assay,75)
```

### no120

```{r}
dtable2(ntd_assay,120)
```

### no121

```{r}
dtable2(ntd_assay,121)
```

### no122

```{r}
dtable2(ntd_assay,122)
```

### no125

```{r}
dtable2(ntd_assay,125)
```

### no126

```{r}
dtable2(ntd_assay,126)
```

### no127

```{r}
dtable2(ntd_assay,127)
```

### no131

```{r}
dtable2(ntd_assay,131)
```

### no136

```{r}
dtable2(ntd_assay,136)
```

### no151

```{r}
dtable2(ntd_assay,151)
```

### no153

```{r}
dtable2(ntd_assay,153)
```

## variance stabilized transformation (vst) {.tabset .tabset-pills}

Conceptually, vst transforms data to make it homoscedastic, that is the variance across ranks (_i.e_ each sample) should be stabilized.

DESeq2 requires > 10000 items/sample to run `vst`, the subset and faster version of variance stabilization. We need to use `varianceStabilizingTransformation` because there are orders of magnitude less items/sample in 16S data.

```{r}
vsd <- varianceStabilizingTransformation(dds)
vsd_assay <- assay(vsd)
colData(vsd)
```

Below are tables for every sample's vst output.

### no1

```{r}
dtable2(vsd_assay, 1)
```

### no3

```{r}
dtable2(vsd_assay,3)
```
### no9

```{r}
dtable2(vsd_assay,9)
```

### no10

```{r}
dtable2(vsd_assay,10)
```

### no15

```{r}
dtable2(vsd_assay,15)
```

### no16

```{r}
dtable2(vsd_assay,16)
```

### no17

```{r}
dtable2(vsd_assay,17)
```

### no18

```{r}
dtable2(vsd_assay,18)
```

### no19

```{r}
dtable2(vsd_assay,19)
```

### no58

```{r}
dtable2(vsd_assay,58)
```

### no60

```{r}
dtable2(vsd_assay,60)
```

### no61

```{r}
dtable2(vsd_assay,61)
```

### no63

```{r}
dtable2(vsd_assay,63)
```

### no65

```{r}
dtable2(vsd_assay,65)
```

### no66

```{r}
dtable2(vsd_assay,66)
```

### no67

```{r}
dtable2(vsd_assay,67)
```

### no69

```{r}
dtable2(vsd_assay,69)
```

### no70

```{r}
dtable2(vsd_assay,70)
```

### no71

```{r}
dtable2(vsd_assay,71)
```

### no72

```{r}
dtable2(vsd_assay,72)
```

### no75

```{r}
dtable2(vsd_assay,75)
```

### no120

```{r}
dtable2(vsd_assay,120)
```

### no121

```{r}
dtable2(vsd_assay,121)
```

### no122

```{r}
dtable2(vsd_assay,122)
```

### no125

```{r}
dtable2(vsd_assay,125)
```

### no126

```{r}
dtable2(vsd_assay,126)
```

### no127

```{r}
dtable2(vsd_assay,127)
```

### no131

```{r}
dtable2(vsd_assay,131)
```

### no136

```{r}
dtable2(vsd_assay,136)
```

### no151

```{r}
dtable2(vsd_assay,151)
```

### no153

```{r}
dtable2(vsd_assay,153)
```

## regularized log transformation (rlog) {.tabset .tabset-pills}

This is similar to $\log_2 (n + n_0)$, where $n$ is the counts and $n_0$ is a positive constant. Here, $n_0$ is varied based on the dispersion-mean trend, in the equation (taken from the DESeq2 vignette, refer to vignette for more details) $\log_2 (q_{ij} = \beta_{i0} + \beta_{ij})$.

```{r}
rld <- rlog(dds)
rld_assay <- assay(rld)
colData(rld)
```

Below are tables for every sample's rlog output.

### no1

```{r}
dtable2(rld_assay, 1)
```

### no3

```{r}
dtable2(rld_assay,3)
```
### no9

```{r}
dtable2(rld_assay,9)
```

### no10

```{r}
dtable2(rld_assay,10)
```

### no15

```{r}
dtable2(rld_assay,15)
```

### no16

```{r}
dtable2(rld_assay,16)
```

### no17

```{r}
dtable2(rld_assay,17)
```

### no18

```{r}
dtable2(rld_assay,18)
```

### no19

```{r}
dtable2(rld_assay,19)
```

### no58

```{r}
dtable2(rld_assay,58)
```

### no60

```{r}
dtable2(rld_assay,60)
```

### no61

```{r}
dtable2(rld_assay,61)
```

### no63

```{r}
dtable2(rld_assay,63)
```

### no65

```{r}
dtable2(rld_assay,65)
```

### no66

```{r}
dtable2(rld_assay,66)
```

### no67

```{r}
dtable2(rld_assay,67)
```

### no69

```{r}
dtable2(rld_assay,69)
```

### no70

```{r}
dtable2(rld_assay,70)
```

### no71

```{r}
dtable2(rld_assay,71)
```

### no72

```{r}
dtable2(rld_assay,72)
```

### no75

```{r}
dtable2(rld_assay,75)
```

### no120

```{r}
dtable2(rld_assay,120)
```

### no121

```{r}
dtable2(rld_assay,121)
```

### no122

```{r}
dtable2(rld_assay,122)
```

### no125

```{r}
dtable2(rld_assay,125)
```

### no126

```{r}
dtable2(rld_assay,126)
```

### no127

```{r}
dtable2(rld_assay,127)
```

### no131

```{r}
dtable2(rld_assay,131)
```

### no136

```{r}
dtable2(rld_assay,136)
```

### no151

```{r}
dtable2(rld_assay,151)
```

### no153

```{r}
dtable2(rld_assay,153)
```

## log10 {.tabset .tabset-pills}

I'll create a custom `DESeqTransform` object from log-transformed counts.

```{r}
esf <- SummarizedExperiment(log(counts(dds)+1), colData = colData(dds)) %>%
  DESeqTransform()
esf_assay <- assay(esf)
colData(esf)
```

Below are tables for every sample's $log_{10}$ output.

### no1

```{r}
dtable2(esf_assay, 1)
```

### no3

```{r}
dtable2(esf_assay,3)
```
### no9

```{r}
dtable2(esf_assay,9)
```

### no10

```{r}
dtable2(esf_assay,10)
```

### no15

```{r}
dtable2(esf_assay,15)
```

### no16

```{r}
dtable2(esf_assay,16)
```

### no17

```{r}
dtable2(esf_assay,17)
```

### no18

```{r}
dtable2(esf_assay,18)
```

### no19

```{r}
dtable2(esf_assay,19)
```

### no58

```{r}
dtable2(esf_assay,58)
```

### no60

```{r}
dtable2(esf_assay,60)
```

### no61

```{r}
dtable2(esf_assay,61)
```

### no63

```{r}
dtable2(esf_assay,63)
```

### no65

```{r}
dtable2(esf_assay,65)
```

### no66

```{r}
dtable2(esf_assay,66)
```

### no67

```{r}
dtable2(esf_assay,67)
```

### no69

```{r}
dtable2(esf_assay,69)
```

### no70

```{r}
dtable2(esf_assay,70)
```

### no71

```{r}
dtable2(esf_assay,71)
```

### no72

```{r}
dtable2(esf_assay,72)
```

### no75

```{r}
dtable2(esf_assay,75)
```

### no120

```{r}
dtable2(esf_assay,120)
```

### no121

```{r}
dtable2(esf_assay,121)
```

### no122

```{r}
dtable2(esf_assay,122)
```

### no125

```{r}
dtable2(esf_assay,125)
```

### no126

```{r}
dtable2(esf_assay,126)
```

### no127

```{r}
dtable2(esf_assay,127)
```

### no131

```{r}
dtable2(esf_assay,131)
```

### no136

```{r}
dtable2(esf_assay,136)
```

### no151

```{r}
dtable2(esf_assay,151)
```

### no153

```{r}
dtable2(esf_assay,153)
```

# visualization and comparisons of normalizations

## hexbin plots

These graphs should help to understand how each transformation is affecting variance in the dataset. Standard deviation is graphed against means. Ideally, variance would be constant for different means and should result in a flat curve. Generally, variance shouldn't be dependent on the mean.

### raw data

```{r}
p0 <- dds_dud %>% assay() %>% meanSdPlot()
```

### ntd

```{r}
p1 <- ntd_assay %>% meanSdPlot()
```

### vst

```{r}
p2 <- vsd_assay %>% meanSdPlot()
```

### rlog

```{r}
p3 <- rld_assay %>% meanSdPlot()
```

### log10

```{r}
p4 <- esf_assay %>% meanSdPlot()
```

```{r, eval=FALSE, echo=FALSE}
fig1 <- ggarrange(p1$gg, p2$gg, p3$gg, p4$gg
                  labels = c("normal transformation", "variance stabilized transformation", "regularized log transformation", "normalized to estimated size factors"),
                  ncol=2, nrow=2)

annotate_figure(
  fig1,
  top = text_grob("Comparisons of normalization methods, variance across means\n",
                  color = "red", face = "bold", size = 14)
  )
```

## count matrix

Taking top 20 counts/estimatedSizeFactors, and generating a pheatmap annotation

```{r}
select <- order(rowMeans(counts(dds,normalized=TRUE)),
                decreasing=TRUE)[1:20]
anno <- as.data.frame(colData(dds)[,c("Gender","Genotype","Time")])
```


### raw data

```{r, fig.width=10}
pheatmap(assay(dds_dud)[select,], cluster_rows=FALSE, show_colnames = FALSE, show_rownames=FALSE,
         cluster_cols=FALSE, annotation_col=anno)
```

### ntd

```{r, fig.width=10}
pheatmap(ntd_assay[select,], cluster_rows=FALSE, show_colnames = FALSE, show_rownames=FALSE,
         cluster_cols=FALSE, annotation_col=anno)
```

### vsd

```{r, fig.width=10}
pheatmap(vsd_assay[select,], cluster_rows=FALSE, show_colnames = FALSE, show_rownames=FALSE,
         cluster_cols=FALSE, annotation_col=anno)
```

### rlog

```{r, fig.width=10}
pheatmap(rld_assay[select,], cluster_rows=FALSE, show_colnames = FALSE, show_rownames=FALSE,
         cluster_cols=FALSE, annotation_col=anno)
```

### log10

```{r, fig.width=10}
pheatmap(esf_assay[select,], cluster_rows=FALSE, show_colnames = FALSE, show_rownames=FALSE,
         cluster_cols=FALSE, annotation_col=anno)
```

## sample-to-sample distance

Procuring a distance matric. It portrays how similar samples are to each others. Darker blue = more similar.

### raw data

```{r}
sampleDists <- dist(t(assay(dds_dud)))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(dds_dud$Gender, dds_dud$Genotype, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors,
         show_rownames = F)
```

### ntd

```{r}
sampleDists <- dist(t(ntd_assay))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(ntd$Gender, ntd$Genotype, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors,
         show_rownames = F)
```

### vsd

```{r}
sampleDists <- dist(t(vsd_assay))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(vsd$Gender, vsd$Genotype, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors,
         show_rownames = F)
```

### rlog

```{r}
sampleDists <- dist(t(rld_assay))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(rld$Gender, rld$Genotype, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors,
         show_rownames = F)
```

### log10

```{r}
sampleDists <- dist(t(esf_assay))
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(esf$Gender, esf$Genotype, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors,
         show_rownames = F)
```

## PCA

### raw data

```{r}
plotPCA(dds_dud, intgroup=c("Gender", "Genotype"))
plotPCA(dds_dud, intgroup=c("Time"))
```

### ntd

```{r}
plotPCA(ntd, intgroup=c("Gender", "Genotype"))
plotPCA(ntd, intgroup=c("Time"))
```

### vst

```{r}
plotPCA(vsd, intgroup=c("Gender", "Genotype"))
plotPCA(vsd, intgroup=c("Time"))
```

### rlog

```{r}
plotPCA(rld, intgroup=c("Gender", "Genotype"))
plotPCA(rld, intgroup=c("Time"))
```

### log10

```{r}
plotPCA(esf, intgroup=c("Gender", "Genotype"))
plotPCA(esf, intgroup=c("Time"))
```

# conclusions

In terms of homoscedasticity, vst leads to the most normalized variance between samples. It also does a better job in reducing sample distance than log transformation alone. rlog had less homoscedasticity compared to vst, but it generated outliers through normalization - it is unclear whether those outliers were generated or all other values were shrunken, but given rlog's method it is most likely that all other values were shrunk. Thus, while sample distances were rendered extremely small, it led to heavy outliers that make it less favorable.  
Log10 transformation alone and ntd were pretty similar, so normalizing to estimated size factors did not lead to any notable difference. Both had more variance relative to the means compared to vst. Distribution was also fine in PCA - no obvious outliers.  
Gratifyingly, all techniques used resulted in the same clustering by pheatmap as the raw counts data, implying that trends in the data were preserved through all transformations.  
In the end, I would rank the normalization techniques, from best normalization of this data to worst with respect to resulting distribution, homoscedasticity, and sample distance, as $vst > ntd \approx log10 >> rlog$.

# session info

```{r, echo=FALSE}
sessionInfo()
